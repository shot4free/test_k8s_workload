---

nginx-ingress:
  enabled: true
  controller:
    autoscaling:
      minReplicas: 10
    nodeSelector:
      cloud.google.com/gke-nodepool: default-2
    resources:
      requests:
        cpu: 1
        memory: 1G

registry:
  hpa:
    # min is set higher until we have more control over the HPA
    # in https://gitlab.com/gitlab-com/gl-infra/delivery/-/issues/1510
    minReplicas: 20
    maxReplicas: 90
  resources:
    requests:
      memory: 3G

gitlab:
  gitlab-shell:
    resources:
      requests:
        cpu: 2000m
    hpa:
      targetAverageValue: 1200m
    minReplicas: 8
    maxReplicas: 150
    nodeSelector:
      cloud.google.com/gke-nodepool: shell-2
  mailroom:
    nodeSelector:
      cloud.google.com/gke-nodepool: default-2
  webservice:
    deployments:
      api:
        hpa:
          minReplicas: 25
          targetAverageValue: 3600m
        workhorse:
          resources:
            limits:
              memory: 2G
            requests:
              cpu: 600m
              memory: 300M
        resources:
          limits:
            memory: 12G
          requests:
            cpu: 4500m
            memory: 7G
      git:
        nodeSelector:
          cloud.google.com/gke-nodepool: git-https-2
      websockets:
        hpa:
          minReplicas: 2
        nodeSelector:
          cloud.google.com/gke-nodepool: websockets-1
        workhorse:
          # these limits are increased due to memory consumption observed when we
          # turned on ActionCable https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3413
          resources:
            limits:
              memory: 4G
            requests:
              memory: 1G
    minReplicas: 30
    maxReplicas: 150
    extraEnv:
      GITLAB_THROTTLE_BYPASS_HEADER: "X-GitLab-RateLimit-Bypass"
      DISABLE_PUMA_NAKAYOSHI_FORK: "true"
      GITLAB_SIDEKIQ_SIZE_LIMITER_MODE: track
      GITLAB_SIDEKIQ_SIZE_LIMITER_LIMIT_BYTES: 100000
  kas:
    nodeSelector:
      cloud.google.com/gke-nodepool: default-2
    workhorse:
      scheme: 'https'
      host: 'int.gprd.gitlab.net'
      port: 443
    customConfig:
      observability:
        sentry:
          dsn: https://af4940ad841b46d78708595fc654af78@sentry.gitlab.net/124
          environment: {{ .Environment.Values | getOrNil "env_prefix" }}
  sidekiq:
    extraEnv:
      SIDEKIQ_SEMI_RELIABLE_FETCH_TIMEOUT: 5
      GITLAB_SIDEKIQ_SIZE_LIMITER_MODE: track
      GITLAB_SIDEKIQ_SIZE_LIMITER_LIMIT_BYTES: 100000
    extraInitContainers: |
      - name: write-instance-name
        args:
          - -c
          - echo "$INSTANCE_NAME" > /etc/gitlab/instance_name
        command:
          - sh
        env:
          - name: INSTANCE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
        image: 'busybox:latest'
        volumeMounts:
          - mountPath: /etc/gitlab
            name: sidekiq-secrets
    psql:
      prepared_statements: false
      load_balancing:
        discover:
          nameserver: <%= File.read('/etc/gitlab/instance_name').strip %>
          record: db-replica.service.consul.
          record_type: SRV
          port: 8600
          use_tcp: true
    pods:
      - name: catchall
        common:
          labels:
            shard: catchall
        concurrency: 15
        minReplicas: 56
        maxReplicas: 175
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-catchall-2
        podLabels:
          shard: catchall
          deployment: sidekiq-catchall
        negateQueues: |
          resource_boundary=cpu&urgency=high&tags!=requires_disk_io|resource_boundary=memory|feature_category=global_search&urgency=throttled|resource_boundary!=cpu&urgency=high|resource_boundary=cpu&urgency=default,low|feature_category=database&urgency=throttled|feature_category=gitaly&urgency=throttled|tags=exclude_from_kubernetes,exclude_from_gitlab_com|name=repository_import&name=authorized_project_update:authorized_project_update_user_refresh_from_replica,authorized_project_update:authorized_project_update_user_refresh_with_low_urgency
        resources:
          requests:
            cpu: 800m
            memory: 1.7G
          limits:
            cpu: 1.5
            memory: 2.5G
        extraEnv:
          ENABLE_LOAD_BALANCING_FOR_SIDEKIQ: "true"
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"catchall\"}"
        extraVolumes: |
          # This is needed because of https://gitlab.com/gitlab-org/gitlab/-/issues/330317
          # where temp files are written to `/srv/gitlab/shared`
          - name: sidekiq-shared
            emptyDir:
              sizeLimit: 10G
      - name: imports
        common:
          labels:
            shard: imports
        concurrency: 1
        minReplicas: 10
        maxReplicas: 10
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-catchall-2
        podLabels:
          deployment: sidekiq-imports
          shard: imports
        queues: name=repository_import
        resources:
          requests:
            cpu: 800m
            memory: 2G
          limits:
            cpu: 1.5
            memory: 4G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"imports\"}"
      - name: memory-bound
        common:
          labels:
            shard: memory-bound
        concurrency: 1
        minReplicas: 8
        maxReplicas: 16
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-memory-bound-3
        podLabels:
          shard: memory-bound
          deployment: sidekiq-memory-bound
        queues: resource_boundary=memory
        resources:
          requests:
            cpu: 500m
            memory: 3G
          limits:
            cpu: 2
            memory: 8G
        extraVolumeMounts: |
          - name: sidekiq-shared
            mountPath: /srv/gitlab/shared
            readOnly: false
        extraVolumes: |
          - name: sidekiq-shared
            emptyDir:
              sizeLimit: 70G
        extraEnv:
          ENABLE_LOAD_BALANCING_FOR_SIDEKIQ: "true"
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"memory-bound\"}"
      # Run background migrations in their own shard
      - name: database-throttled
        common:
          labels:
            shard: database-throttled
        concurrency: 5 # Discussion on this value in https://gitlab.com/gitlab-com/gl-infra/k8s-workloads/gitlab-com/-/merge_requests/276/diffs#note_367270352
        minReplicas: 1
        maxReplicas: 1
        nodeSelector:
          cloud.google.com/gke-nodepool: default-2
        podLabels:
          shard: database-throttled
          deployment: sidekiq-database-throttled
        queues: feature_category=database&urgency=throttled
        resources:
          requests:
            cpu: 500m
            memory: 1300M
          limits:
            cpu: 1.5
            memory: 6G
        extraEnv:
          ENABLE_LOAD_BALANCING_FOR_SIDEKIQ: "true"
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"database-throttled\"}"
      # Run Gitaly Storage Migrations on their own shards
      # https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/436
      # Allow up to a maximum of 24 concurrent gitaly-throttled jobs
      - name: gitaly-throttled
        common:
          labels:
            shard: gitaly-throttled
        concurrency: 8
        minReplicas: 1
        maxReplicas: 3
        nodeSelector:
          cloud.google.com/gke-nodepool: default-2
        podLabels:
          shard: gitaly-throttled
          deployment: sidekiq-gitaly-throttled
        queues: feature_category=gitaly&urgency=throttled
        resources:
          requests:
            cpu: 500m
            memory: 1300M
          limits:
            cpu: 1.5
            memory: 6G
        extraEnv:
          ENABLE_LOAD_BALANCING_FOR_SIDEKIQ: "true"
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"gitaly-throttled\"}"
      - name: elasticsearch
        common:
          labels:
            shard: elasticsearch
        concurrency: 2
        minReplicas: 2
        maxReplicas: 8
        nodeSelector:
          cloud.google.com/gke-nodepool: default-2
        podLabels:
          shard: elasticsearch
          deployment: sidekiq-elasticsearch
        queues: feature_category=global_search&urgency=throttled
        resources:
          requests:
            cpu: 800m
            memory: 2G
          limits:
            cpu: 2
            memory: 8G
        extraEnv:
          ENABLE_LOAD_BALANCING_FOR_SIDEKIQ: "true"
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"elasticsearch\"}"
      # Production resources for this shard discussed in
      # https://gitlab.com/gitlab-com/gl-infra/infrastructure/-/issues/10445
      # https://gitlab.com/gitlab-com/gl-infra/delivery/-/issues/901
      - name: low-urgency-cpu-bound
        common:
          labels:
            shard: low-urgency-cpu-bound
        concurrency: 5
        minReplicas: 10
        maxReplicas: 100
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-low-urgency-cpu-bound-2
        podLabels:
          shard: low-urgency-cpu-bound
          deployment: sidekiq-low-urgency-cpu-bound
        queues: resource_boundary=cpu&urgency=default,low
        resources:
          requests:
            cpu: 1
            memory: 5G
          limits:
            cpu: 1.5
            memory: 6G
        extraEnv:
          ENABLE_LOAD_BALANCING_FOR_SIDEKIQ: "true"
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"low-urgency-cpu-bound\"}"
      # This is a shard that segregates seemingly problematic queues
      - name: quarantine
        common:
          labels:
            shard: quarantine
        concurrency: 15
        minReplicas: 10
        maxReplicas: 50
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-catchall-2
        podLabels:
          shard: quarantine
          deployment: sidekiq-quarantine
        queues: name=authorized_project_update:authorized_project_update_user_refresh_from_replica,authorized_project_update:authorized_project_update_user_refresh_with_low_urgency
        resources:
          requests:
            cpu: 800m
            memory: 1.7G
          limits:
            cpu: 1.5
            memory: 2.5G
        extraEnv:
          ENABLE_LOAD_BALANCING_FOR_SIDEKIQ: "true"
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"quarantine\"}"
      # Production resources for urgent-cpu-bound discussed in
      # https://gitlab.com/gitlab-com/gl-infra/infrastructure/-/issues/10639
      - name: urgent-cpu-bound
        common:
          labels:
            shard: urgent-cpu-bound
        concurrency: 5
        hpa:
          targetAverageValue: 500m
        minReplicas: 30
        maxReplicas: 105
        memoryKiller:
          checkInterval: 3
          graceTime: 900
          maxRss: 3000000
          shutdownWait: 30
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-urgent-cpu-bound-2
        podLabels:
          shard: urgent-cpu-bound
          deployment: sidekiq-urgent-cpu-bound
        queues: resource_boundary=cpu&urgency=high&tags!=requires_disk_io
        resources:
          requests:
            cpu: 650m
            memory: 2.5G
          limits:
            cpu: 2
            memory: 3.1G
        extraEnv:
          ENABLE_LOAD_BALANCING_FOR_SIDEKIQ: "true"
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"urgent-cpu-bound\"}"
      # Production resources for this shard discussed in
      # https://gitlab.com/gitlab-com/gl-infra/infrastructure/-/issues/10446
      - name: urgent-other
        common:
          labels:
            shard: urgent-other
        concurrency: 5
        minReplicas: 175
        maxReplicas: 175
        memoryKiller:
          checkInterval: 3
          graceTime: 900
          maxRss: 2900000
          shutdownWait: 30
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-urgent-other-3
        podLabels:
          shard: urgent-other
          deployment: sidekiq-urgent-other
        queues: resource_boundary!=cpu&urgency=high
        resources:
          requests:
            cpu: 500m
            memory: 2G
          limits:
            cpu: 1.5
            memory: 3G
        extraEnv:
          ENABLE_LOAD_BALANCING_FOR_SIDEKIQ: "true"
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"urgent-other\"}"

global:
  appConfig:
    omniauth:
      providers:
        - secret: gitlab-google-oauth2-v1
        - secret: gitlab-twitter-oauth2-v1
        - secret: gitlab-github-oauth2-v1
        - secret: gitlab-bitbucket-oauth2-v1
        - secret: gitlab-group-saml-oauth2-v1
        - secret: gitlab-salesforce-oauth2-v1

    sidekiq:
      routingRules:
        - ["name=project_import_schedule", null] # we cannot migrate this worker yet, https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/1064
        - ["name=email_receiver,service_desk_email_receiver", null] # we need to change mail_room config to migrate these, https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/1087
        - ["resource_boundary=cpu&urgency=high&tags!=requires_disk_io", null] # urgent-cpu-bound
        - ["resource_boundary=memory", null] # memory-bound
        - ["feature_category=global_search&urgency=throttled", null] # elasticsearch
        - ["resource_boundary!=cpu&urgency=high", null] # urgent-other
        - ["resource_boundary=cpu&urgency=default,low", null] # low-urgency-cpu-bound
        - ["feature_category=database&urgency=throttled", null] # database-throttled
        - ["feature_category=gitaly&urgency=throttled", null] # gitaly-throttled
        - ["tags=exclude_from_kubernetes", null] # catchall on VMs
        - ["tags=exclude_from_gitlab_com", "default"] # special case for testing (only runs actively on staging; includes geo jobs)
        - ["*", null] # catchall on k8s

  hosts:
    gitlab:
      name: gitlab.com
    kas:
      name: kas.gitlab.com

  psql:
    prepared_statements: false
    load_balancing:
      discover:
        nameserver: <%= File.read('/etc/gitlab/instance_name').strip %>
        record: db-replica.service.consul.
        record_type: SRV
        port: 8600
        use_tcp: true
