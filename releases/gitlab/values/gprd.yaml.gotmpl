---

nginx-ingress:
  controller:
    autoscaling:
      minReplicas: 10
    nodeSelector:
      cloud.google.com/gke-nodepool: default-2
    resources:
      requests:
        cpu: 1
        memory: 1G

registry:
  hpa:
    # min is set higher until we have more control over the HPA
    # in https://gitlab.com/gitlab-com/gl-infra/delivery/-/issues/1510
    minReplicas: 20
    maxReplicas: 90
  resources:
    requests:
      memory: 3G
  storage:
    secret: registry-storage-v3
  # When enabled, the upload purger will attempt to make deletes against the
  # common GCS bucket. It is not suitable to be enabled in production until the
  # following issues are resolved:
  # https://gitlab.com/gitlab-org/container-registry/-/issues/216
  # https://gitlab.com/gitlab-org/container-registry/-/issues/217
  maintenance:
    uploadpurging:
      enabled: false

gitlab:
  gitlab-shell:
    resources:
      requests:
        cpu: 2000m
    hpa:
      targetAverageValue: 1200m
    minReplicas: 8
    maxReplicas: 150
    nodeSelector:
      cloud.google.com/gke-nodepool: shell-2
  mailroom:
    nodeSelector:
      cloud.google.com/gke-nodepool: default-2
  webservice:
    deployments:
      api:
        hpa:
          minReplicas: 25
          targetAverageValue: 3600m
        workhorse:
          resources:
            limits:
              memory: 2G
            requests:
              cpu: 600m
              memory: 300M
        resources:
          limits:
            memory: 12G
          requests:
            cpu: 4500m
            memory: 7G
      git:
        nodeSelector:
          cloud.google.com/gke-nodepool: git-https-2
      web:
        hpa:
          minReplicas: 25
          maxReplicas: 150
          targetAverageValue: 2200m
        nodeSelector:
          cloud.google.com/gke-nodepool: web-0
        resources:
          limits:
            memory: 8G
      websockets:
        hpa:
          minReplicas: 2
        nodeSelector:
          cloud.google.com/gke-nodepool: websockets-1
        workhorse:
          # these limits are increased due to memory consumption observed when we
          # turned on ActionCable https://gitlab.com/gitlab-com/gl-infra/production/-/issues/3413
          resources:
            limits:
              memory: 4G
            requests:
              memory: 1G
    minReplicas: 30
    maxReplicas: 150
    extraEnv:
      GITLAB_THROTTLE_BYPASS_HEADER: "X-GitLab-RateLimit-Bypass"
      DISABLE_PUMA_NAKAYOSHI_FORK: "true"
      GITLAB_SIDEKIQ_SIZE_LIMITER_MODE: compress
      GITLAB_SIDEKIQ_SIZE_LIMITER_LIMIT_BYTES: "5000000"
      GITLAB_SIDEKIQ_SIZE_LIMITER_COMPRESSION_THRESHOLD_BYTES: "100000"
      GITLAB_PERFORMANCE_BAR_STATS_URL: "https://log.gprd.gitlab.net/app/dashboards#/view/823ed530-655a-11eb-a318-db81ec4462fa?_g=(filters%3A!()%2CrefreshInterval%3A(pause%3A!t%2Cvalue%3A0)%2Ctime%3A(from%3Anow-24h%2Cto%3Anow))"
  kas:
    nodeSelector:
      cloud.google.com/gke-nodepool: default-2
    workhorse:
      scheme: 'https'
      host: 'int.gprd.gitlab.net'
      port: 443
    customConfig:
      observability:
        sentry:
          dsn: https://af4940ad841b46d78708595fc654af78@sentry.gitlab.net/124
          environment: {{ .Environment.Values | getOrNil "env_prefix" }}
  sidekiq:
    extraEnv:
      SIDEKIQ_SEMI_RELIABLE_FETCH_TIMEOUT: 5
      GITLAB_SIDEKIQ_SIZE_LIMITER_MODE: compress
      GITLAB_SIDEKIQ_SIZE_LIMITER_LIMIT_BYTES: "5000000"
      GITLAB_SIDEKIQ_SIZE_LIMITER_COMPRESSION_THRESHOLD_BYTES: "100000"
    extraInitContainers: |
      - name: write-instance-name
        args:
          - -c
          - echo "$INSTANCE_NAME" > /etc/gitlab/instance_name
        command:
          - sh
        env:
          - name: INSTANCE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
        image: 'busybox:latest'
        volumeMounts:
          - mountPath: /etc/gitlab
            name: sidekiq-secrets
    psql:
      host: pgbouncer-sidekiq.int.gprd.gitlab.net
      prepared_statements: false
      load_balancing:
        discover:
          nameserver: <%= File.read('/etc/gitlab/instance_name').strip %>
          record: db-replica.service.consul.
          record_type: SRV
          port: 8600
          use_tcp: true
    pods:
      - name: catchall
        common:
          labels:
            shard: catchall
        hpa:
          targetAverageValue: 600m
        concurrency: 15
        minReplicas: 56
        maxReplicas: 175
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-catchall-2
        podLabels:
          shard: catchall
          deployment: sidekiq-catchall
        queueSelector: false
        queues: default,mailers,project_import_schedule,service_desk_email_receiver
        resources:
          requests:
            cpu: 800m
            memory: 1.7G
          limits:
            cpu: 1.5
            memory: 2.5G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"catchall\"}"
        extraVolumes: |
          # This is needed because of https://gitlab.com/gitlab-org/gitlab/-/issues/330317
          # where temp files are written to `/srv/gitlab/shared`
          - name: sidekiq-shared
            emptyDir:
              sizeLimit: 10G
      - name: imports
        common:
          labels:
            shard: imports
        concurrency: 1
        minReplicas: 10
        maxReplicas: 10
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-catchall-2
        podLabels:
          deployment: sidekiq-imports
          shard: imports
        queues: name=repository_import
        resources:
          requests:
            cpu: 800m
            memory: 2G
          limits:
            cpu: 1.5
            memory: 4G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"imports\"}"
      - name: memory-bound
        common:
          labels:
            shard: memory-bound
        concurrency: 1
        minReplicas: 8
        maxReplicas: 16
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-memory-bound-3
        podLabels:
          shard: memory-bound
          deployment: sidekiq-memory-bound
        queueSelector: false
        queues: memory_bound,design_management_new_version,project_export
        resources:
          requests:
            cpu: 500m
            memory: 3G
          limits:
            cpu: 2
            memory: 8G
        extraVolumeMounts: |
          - name: sidekiq-shared
            mountPath: /srv/gitlab/shared
            readOnly: false
        extraVolumes: |
          - name: sidekiq-shared
            emptyDir:
              sizeLimit: 70G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"memory-bound\"}"
      # Run background migrations in their own shard
      - name: database-throttled
        common:
          labels:
            shard: database-throttled
        concurrency: 5 # Discussion on this value in https://gitlab.com/gitlab-com/gl-infra/k8s-workloads/gitlab-com/-/merge_requests/276/diffs#note_367270352
        minReplicas: 1
        maxReplicas: 1
        nodeSelector:
          cloud.google.com/gke-nodepool: default-2
        podLabels:
          shard: database-throttled
          deployment: sidekiq-database-throttled
        queues: feature_category=database&urgency=throttled
        resources:
          requests:
            cpu: 500m
            memory: 1300M
          limits:
            cpu: 1.5
            memory: 6G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"database-throttled\"}"
      # Run Gitaly Storage Migrations on their own shards
      # https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/436
      # Allow up to a maximum of 24 concurrent gitaly-throttled jobs
      - name: gitaly-throttled
        common:
          labels:
            shard: gitaly-throttled
        concurrency: 8
        minReplicas: 1
        maxReplicas: 3
        nodeSelector:
          cloud.google.com/gke-nodepool: default-2
        podLabels:
          shard: gitaly-throttled
          deployment: sidekiq-gitaly-throttled
        queues: feature_category=gitaly&urgency=throttled
        resources:
          requests:
            cpu: 500m
            memory: 1300M
          limits:
            cpu: 1.5
            memory: 6G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"gitaly-throttled\"}"
      - name: elasticsearch
        common:
          labels:
            shard: elasticsearch
        concurrency: 2
        minReplicas: 2
        maxReplicas: 8
        nodeSelector:
          cloud.google.com/gke-nodepool: default-2
        podLabels:
          shard: elasticsearch
          deployment: sidekiq-elasticsearch
        queueSelector: false
        queues: "elasticsearch,cronjob:elastic_cluster_reindexing_cron,cronjob:elastic_index_bulk_cron,cronjob:elastic_index_initial_bulk_cron,cronjob:elastic_migration,elastic_commit_indexer,elastic_delete_project"
        resources:
          requests:
            cpu: 800m
            memory: 2G
          limits:
            cpu: 2
            memory: 8G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"elasticsearch\"}"
      # Production resources for this shard discussed in
      # https://gitlab.com/gitlab-com/gl-infra/infrastructure/-/issues/10445
      # https://gitlab.com/gitlab-com/gl-infra/delivery/-/issues/901
      - name: low-urgency-cpu-bound
        common:
          labels:
            shard: low-urgency-cpu-bound
        concurrency: 5
        minReplicas: 10
        maxReplicas: 100
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-low-urgency-cpu-bound-2
        podLabels:
          shard: low-urgency-cpu-bound
          deployment: sidekiq-low-urgency-cpu-bound
        queues: resource_boundary=cpu&urgency=default,low
        resources:
          requests:
            cpu: 1
            memory: 5G
          limits:
            cpu: 1.5
            memory: 6G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"low-urgency-cpu-bound\"}"
      # This is a shard that segregates seemingly problematic queues
      - name: quarantine
        common:
          labels:
            shard: quarantine
        concurrency: 15
        minReplicas: 10
        maxReplicas: 50
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-catchall-2
        podLabels:
          shard: quarantine
          deployment: sidekiq-quarantine
        queues: name=authorized_project_update:authorized_project_update_user_refresh_from_replica,authorized_project_update:authorized_project_update_user_refresh_with_low_urgency
        resources:
          requests:
            cpu: 800m
            memory: 1.7G
          limits:
            cpu: 1.5
            memory: 2.5G
        extraEnv:
          ENABLE_LOAD_BALANCING_FOR_SIDEKIQ: "true"
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"quarantine\"}"
      # Production resources for urgent-cpu-bound discussed in
      # https://gitlab.com/gitlab-com/gl-infra/infrastructure/-/issues/10639
      - name: urgent-cpu-bound
        common:
          labels:
            shard: urgent-cpu-bound
        concurrency: 5
        hpa:
          targetAverageValue: 500m
        minReplicas: 30
        maxReplicas: 105
        memoryKiller:
          checkInterval: 3
          graceTime: 900
          maxRss: 3000000
          shutdownWait: 30
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-urgent-cpu-bound-2
        podLabels:
          shard: urgent-cpu-bound
          deployment: sidekiq-urgent-cpu-bound
        queueSelector: false
        queues: urgent_cpu_bound,incident_management_pending_escalations_alert_check,incident_management_pending_escalations_alert_create,issue_placement,merge_request_reset_approvals,new_issue,new_merge_request,new_note,pipeline_background:ci_sync_reports_to_report_approval_rules,pipeline_cache:expire_pipeline_cache,pipeline_creation:ci_external_pull_requests_create_pipeline,pipeline_creation:create_pipeline,pipeline_creation:merge_requests_create_pipeline,pipeline_default:ci_pipeline_bridge_status,pipeline_default:ci_retry_pipeline,pipeline_default:pipeline_notification,pipeline_processing:build_finished,pipeline_processing:build_queue,pipeline_processing:ci_build_finished,pipeline_processing:update_head_pipeline_for_merge_request,post_receive,update_merge_requests
        resources:
          requests:
            cpu: 650m
            memory: 2.5G
          limits:
            cpu: 2
            memory: 3.1G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"urgent-cpu-bound\"}"
      # Production resources for this shard discussed in
      # https://gitlab.com/gitlab-com/gl-infra/infrastructure/-/issues/10446
      - name: urgent-other
        common:
          labels:
            shard: urgent-other
        concurrency: 5
        minReplicas: 175
        maxReplicas: 175
        memoryKiller:
          checkInterval: 3
          graceTime: 900
          maxRss: 2900000
          shutdownWait: 30
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-urgent-other-3
        podLabels:
          shard: urgent-other
          deployment: sidekiq-urgent-other
        queueSelector: false
        queues: urgent_other,email_receiver,authorized_keys,authorized_project_update:authorized_project_update_project_recalculate,authorized_projects,gitlab_shell,merge,merge_requests_delete_source_branch,merge_requests_handle_assignees_change,merge_requests_resolve_todos,merge_requests_sync_code_owner_approval_rules,pipeline_cache:expire_job_cache,pipeline_default:pipeline_metrics,pipeline_hooks:build_hooks,pipeline_processing:build_success,pipeline_processing:ci_initial_pipeline_process,pipeline_processing:pipeline_process,pipeline_processing:stage_update,process_commit,project_cache,prometheus_create_default_alerts,update_highest_role
        resources:
          requests:
            cpu: 500m
            memory: 2G
          limits:
            cpu: 1.5
            memory: 3G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"urgent-other\"}"

global:
  appConfig:
    contentSecurityPolicy:
      enabled: true
      report_only: false
      directives:
        connect_src: "'self' https://gitlab.com https://assets.gitlab-static.net wss://gitlab.com https://sentry.gitlab.net https://customers.gitlab.com https://snowplow.trx.gitlab.net https://sourcegraph.com https://ec2.ap-east-1.amazonaws.com https://ec2.ap-northeast-1.amazonaws.com https://ec2.ap-northeast-2.amazonaws.com https://ec2.ap-northeast-3.amazonaws.com https://ec2.ap-south-1.amazonaws.com https://ec2.ap-southeast-1.amazonaws.com https://ec2.ap-southeast-2.amazonaws.com https://ec2.ca-central-1.amazonaws.com https://ec2.eu-central-1.amazonaws.com https://ec2.eu-north-1.amazonaws.com https://ec2.eu-west-1.amazonaws.com https://ec2.eu-west-2.amazonaws.com https://ec2.eu-west-3.amazonaws.com https://ec2.me-south-1.amazonaws.com https://ec2.sa-east-1.amazonaws.com https://ec2.us-east-1.amazonaws.com https://ec2.us-east-2.amazonaws.com https://ec2.us-west-1.amazonaws.com https://ec2.us-west-2.amazonaws.com https://ec2.af-south-1.amazonaws.com https://iam.amazonaws.com"
        frame_ancestors: "'self'"
        frame_src: "'self' https://assets.gitlab-static.net https://www.google.com/recaptcha/ https://www.recaptcha.net/ https://content.googleapis.com https://content-cloudresourcemanager.googleapis.com https://content-compute.googleapis.com https://content-cloudbilling.googleapis.com https://*.codesandbox.io https://customers.gitlab.com"
        img_src: "* data: blob:"
        object_src: "'none'"
        script_src: "'self' 'unsafe-inline' 'unsafe-eval' https://assets.gitlab-static.net https://www.google.com/recaptcha/ https://www.gstatic.com/recaptcha/ https://www.recaptcha.net/ https://apis.google.com"
        style_src: "'self' 'unsafe-inline' https://assets.gitlab-static.net"
        worker_src: "https://assets.gitlab-static.net https://gitlab.com blob: data:"
    omniauth:
      providers:
        - secret: gitlab-google-oauth2-v1
        - secret: gitlab-twitter-oauth2-v1
        - secret: gitlab-github-oauth2-v1
        - secret: gitlab-bitbucket-oauth2-v1
        - secret: gitlab-group-saml-oauth2-v1
        - secret: gitlab-salesforce-oauth2-v1

    sidekiq:
      routingRules:
        - ["worker_name=ProjectImportScheduleWorker", null] # we cannot migrate this worker yet, https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/1064
        - ["worker_name=EmailReceiverWorker,ServiceDeskEmailReceiverWorker", null] # we need to change mail_room config to migrate these, https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/1087
        - ["worker_name=AuthorizedProjectUpdate::UserRefreshFromReplicaWorker,AuthorizedProjectUpdate::UserRefreshWithLowUrgencyWorker", null] # move this to the quarantine shard
        - ["resource_boundary=cpu&urgency=high", "urgent_cpu_bound"] # urgent-cpu-bound
        - ["resource_boundary=memory", null] # memory-bound
        - ["feature_category=global_search&urgency=throttled", "elasticsearch"] # elasticsearch
        - ["resource_boundary!=cpu&urgency=high", "urgent_other"] # urgent-other
        - ["resource_boundary=cpu&urgency=default,low", null] # low-urgency-cpu-bound
        - ["feature_category=database&urgency=throttled", null] # database-throttled
        - ["feature_category=gitaly&urgency=throttled", null] # gitaly-throttled
        - ["tags=exclude_from_gitlab_com", "default"] # special case for testing (only runs actively on staging; includes geo jobs)
        - ["feature_category=not_owned", "default"] # Iterating to full 'default' routing for catchall
        - ["feature_category=auto_devops,chatops,code_analytics,compliance_management,container_registry,database", "default"] # Phase 2 default routing for catchall
        - ["feature_category=design_management,devops_reports,epics,geo_replication,global_search,kubernetes_management", "default"] # Phase 2 default routing for catchall
        - ["feature_category=license,license_compliance,metrics,package_registry,product_analytics,release_evidence", "default"] # Phase 2 default routing for catchall
        - ["feature_category=security_orchestration,service_desk,templates,users,utilization,vulnerability_management", "default"] # Phase 2 default routing for catchall
        - ["feature_category=code_review,incident_management,importers,pages", "default"] # Phase 3 default routing for catchall
        - ["feature_category=code_testing,continuous_delivery,subgroups,authentication_and_authorization,gitaly,issue_tracking,requirements_management", "default"] # Phase 4 default routing for catchall
        - ["feature_category=continuous_integration", "default"] # Phase 5 default routing for catchall
        - ["feature_category=integrations,dynamic_application_security_testing", "default"] # Phase 6 default routing for catchall
        - ["feature_category=source_code_management", "default"] # Phase 7 default routing for catchall
        - ["*", "default"] # catchall on k8s

  hosts:
    gitlab:
      name: gitlab.com
    kas:
      name: kas.gitlab.com

  psql:
    prepared_statements: false
    load_balancing:
      discover:
        nameserver: <%= File.read('/etc/gitlab/instance_name').strip %>
        record: db-replica.service.consul.
        record_type: SRV
        port: 8600
        use_tcp: true
