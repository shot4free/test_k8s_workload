---

nginx-ingress:
  controller:
    nodeSelector:
      cloud.google.com/gke-nodepool: default-2

registry:
  image:
    tag: v3.5.2-gitlab

gitlab:
  mailroom:
    nodeSelector:
      cloud.google.com/gke-nodepool: default-2
  webservice:
    deployments:
      api:
        nodeSelector:
          cloud.google.com/gke-nodepool: api-2
      web:
        extraEnv:
          CUSTOMER_PORTAL_URL: "https://customers.stg.gitlab.com"
          GITLAB_PERFORMANCE_BAR_STATS_URL: "https://nonprod-log.gitlab.net/app/dashboards#/view/8d301370-72b4-11eb-9f15-4952965e0e78?_g=(filters%3A!()%2CrefreshInterval%3A(pause%3A!t%2Cvalue%3A0)%2Ctime%3A(from%3Anow-15m%2Cto%3Anow))"
          GITLAB_TRACING: "opentracing://jaeger?udp_endpoint=localhost%3A6831&sampler=probabilistic&sampler_param=0.01&service_name=web"
          GITLAB_TRACING_URL: 'https://jaeger.gstg.gitlab.net/search?service={{ "{{" }} service {{ "}}" }}&tags=%7B{{ "\"" }}correlation_id{{ "\"" }}%3A{{ "\""}}{{ "{{" }} correlation_id {{ "}}" }}{{ "\"" }}%7D'
    minReplicas: 2
    maxReplicas: 30
    extraEnv:
      GITLAB_THROTTLE_BYPASS_HEADER: "X-GitLab-RateLimit-Bypass"
      GITLAB_THROTTLE_DRY_RUN: "throttle_unauthenticated,throttle_authenticated_api,throttle_authenticated_web"
      DISABLE_PUMA_NAKAYOSHI_FORK: "true"
      GITLAB_SIDEKIQ_SIZE_LIMITER_MODE: compress
      GITLAB_SIDEKIQ_SIZE_LIMITER_LIMIT_BYTES: "5000000"
      GITLAB_SIDEKIQ_SIZE_LIMITER_COMPRESSION_THRESHOLD_BYTES: "100000"
      USE_NEW_LOAD_BALANCER_QUERY: "true"
    workhorse:
      resources:
        limits:
          memory: 2G
        requests:
          cpu: 600m
          memory: 200M
    resources:
      limits:
        memory: 6.0G
      requests:
        cpu: 4
        memory: 5G
  kas:
    nodeSelector:
      cloud.google.com/gke-nodepool: default-2
    workhorse:
      scheme: 'https'
      host: 'int.gstg.gitlab.net'
      port: 11443
    customConfig:
      observability:
        sentry:
          dsn: https://af4940ad841b46d78708595fc654af78@sentry.gitlab.net/124
          environment: {{ .Environment.Values | getOrNil "env_prefix" }}
  sidekiq:
    extraEnv:
      SIDEKIQ_SEMI_RELIABLE_FETCH_TIMEOUT: 5
      ENABLE_LOAD_BALANCING_FOR_SIDEKIQ: "true"
      USE_GITLAB_LOGGER: 1
      GITLAB_SIDEKIQ_SIZE_LIMITER_MODE: compress
      GITLAB_SIDEKIQ_SIZE_LIMITER_LIMIT_BYTES: "5000000"
      GITLAB_SIDEKIQ_SIZE_LIMITER_COMPRESSION_THRESHOLD_BYTES: "100000"
    extraInitContainers: |
      - name: write-instance-name
        args:
          - -c
          - echo "$INSTANCE_NAME" > /etc/gitlab/instance_name
        command:
          - sh
        env:
          - name: INSTANCE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
        image: 'busybox:latest'
        volumeMounts:
          - mountPath: /etc/gitlab
            name: sidekiq-secrets
    pods:
      - name: catchall
        common:
          labels:
            shard: catchall
        concurrency: 15
        minReplicas: 1
        maxReplicas: 100
        nodeSelector:
          cloud.google.com/gke-nodepool: catchall-0
        podLabels:
          deployment: sidekiq-catchall
          shard: catchall
        queueSelector: false
        queues: "default,mailers,project_import_schedule"
        resources:
          requests:
            cpu: 800m
            memory: 1G
          limits:
            cpu: 1.5
            memory: 2G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"catchall\"}"
        extraVolumes: |
          # This is needed because of https://gitlab.com/gitlab-org/gitlab/-/issues/330317
          # where temp files are written to `/srv/gitlab/shared`
          - name: sidekiq-shared
            emptyDir:
              sizeLimit: 10G
      - name: imports
        common:
          labels:
            shard: imports
        concurrency: 1
        minReplicas: 2
        maxReplicas: 2
        nodeSelector:
          cloud.google.com/gke-nodepool: catchall-0
        podLabels:
          deployment: sidekiq-imports
          shard: imports
        queues: name=repository_import
        resources:
          requests:
            cpu: 800m
            memory: 1G
          limits:
            cpu: 1.5
            memory: 2G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"imports\"}"
      - name: memory-bound
        common:
          labels:
            shard: memory-bound
        concurrency: 1
        minReplicas: 1
        maxReplicas: 16
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-memory-bound-1
        podLabels:
          deployment: sidekiq-memory-bound
          shard: memory-bound
        queues: resource_boundary=memory
        resources:
          requests:
            cpu: 500m
            memory: 3G
          limits:
            cpu: 2
            memory: 8G
        extraVolumeMounts: |
          - name: sidekiq-shared
            mountPath: /srv/gitlab/shared
            readOnly: false
        extraVolumes: |
          - name: sidekiq-shared
            emptyDir:
              sizeLimit: 50G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"memory-bound\"}"
      # Run background migrations in their own shard
      - name: database-throttled
        common:
          labels:
            shard: database-throttled
        concurrency: 5 # Discussion on this value in https://gitlab.com/gitlab-com/gl-infra/k8s-workloads/gitlab-com/-/merge_requests/276/diffs#note_367270352
        minReplicas: 1
        maxReplicas: 1
        nodeSelector:
          cloud.google.com/gke-nodepool: default-2
        podLabels:
          deployment: sidekiq-database-throttled
          shard: database-throttled
        queues: feature_category=database&urgency=throttled
        resources:
          requests:
            cpu: 500m
            memory: 1300M
          limits:
            cpu: 1.5
            memory: 6G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"database-throttled\"}"
      # Run Gitaly Storage Migrations on their own shards
      # https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/436
      # Allow up to a maximum of 24 concurrent gitaly-throttled jobs
      - name: gitaly-throttled
        common:
          labels:
            shard: gitaly-throttled
        concurrency: 8
        minReplicas: 1
        maxReplicas: 3
        nodeSelector:
          cloud.google.com/gke-nodepool: default-2
        podLabels:
          deployment: sidekiq-gitaly-throttled
          shard: gitaly-throttled
        queues: feature_category=gitaly&urgency=throttled
        resources:
          requests:
            cpu: 500m
            memory: 1300M
          limits:
            cpu: 1.5
            memory: 6G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"gitaly-throttled\"}"
      - name: elasticsearch
        common:
          labels:
            shard: elasticsearch
        concurrency: 2
        minReplicas: 2
        maxReplicas: 2
        nodeSelector:
          cloud.google.com/gke-nodepool: default-2
        podLabels:
          deployment: sidekiq-elasticsearch
          shard: elasticsearch
        queues: feature_category=global_search&urgency=throttled
        resources:
          requests:
            cpu: 800m
            memory: 2G
          limits:
            cpu: 2
            memory: 8G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"elasticsearch\"}"
      - name: low-urgency-cpu-bound
        common:
          labels:
            shard: low-urgency-cpu-bound
        concurrency: 5
        minReplicas: 2
        maxReplicas: 10
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-low-urgency-cpu-bound-1
        podLabels:
          deployment: sidekiq-low-urgency-cpu-bound
          shard: low-urgency-cpu-bound
        queues: resource_boundary=cpu&urgency=default,low
        resources:
          requests:
            cpu: 1
            memory: 5G
          limits:
            cpu: 1.5
            memory: 6G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"low-urgency-cpu-bound\"}"
      # This is a shard that segregates seemingly problematic queues from the rest of our infrastructure
      - name: quarantine
        common:
          labels:
            shard: quarantine
        concurrency: 15
        minReplicas: 1
        maxReplicas: 50
        nodeSelector:
          cloud.google.com/gke-nodepool: catchall-0
        podLabels:
          deployment: sidekiq-quarantine
          shard: quarantine
        queues: name=authorized_project_update:authorized_project_update_user_refresh_from_replica,authorized_project_update:authorized_project_update_user_refresh_with_low_urgency
        resources:
          requests:
            cpu: 800m
            memory: 1G
          limits:
            cpu: 1.5
            memory: 2G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"quarantine\"}"
      - name: urgent-cpu-bound
        common:
          labels:
            shard: urgent-cpu-bound
        concurrency: 5
        minReplicas: 1
        maxReplicas: 10
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-urgent-cpu-bound-1
        podLabels:
          deployment: sidekiq-urgent-cpu-bound
          shard: urgent-cpu-bound
        queues: resource_boundary=cpu&urgency=high&tags!=requires_disk_io
        resources:
          requests:
            cpu: 650m
            memory: 2.5G
          limits:
            cpu: 1
            memory: 3.1G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"urgent-cpu-bound\"}"
      - name: urgent-other
        common:
          labels:
            shard: urgent-other
        concurrency: 5
        minReplicas: 5
        maxReplicas: 10
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-urgent-other-1
        podLabels:
          deployment: sidekiq-urgent-other
          shard: urgent-other
        queues: resource_boundary!=cpu&urgency=high
        resources:
          requests:
            cpu: 500m
            memory: 2G
          limits:
            cpu: 1.5
            memory: 3G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"urgent-other\"}"
    psql:
      load_balancing:
        discover:
          nameserver: <%= File.read('/etc/gitlab/instance_name').strip %>
          record: db-replica.service.consul.
          record_type: SRV
          port: 8600
          use_tcp: true

global:
  appConfig:
    contentSecurityPolicy:
      enabled: true
      report_only: false
      directives:
        connect_src: "'self' https://staging.gitlab.com https://gl-staging.freetls.fastly.net https://gl-staging-canary.freetls.fastly.net wss://gitlab.com wss://staging.gitlab.com wss://gstg.gitlab.com https://sentry.gitlab.net https://customers.stg.gitlab.com https://snowplow.trx.gitlab.net https://sourcegraph.com https://ec2.ap-east-1.amazonaws.com https://ec2.ap-northeast-1.amazonaws.com https://ec2.ap-northeast-2.amazonaws.com https://ec2.ap-northeast-3.amazonaws.com https://ec2.ap-south-1.amazonaws.com https://ec2.ap-southeast-1.amazonaws.com https://ec2.ap-southeast-2.amazonaws.com https://ec2.ca-central-1.amazonaws.com https://ec2.eu-central-1.amazonaws.com https://ec2.eu-north-1.amazonaws.com https://ec2.eu-west-1.amazonaws.com https://ec2.eu-west-2.amazonaws.com https://ec2.eu-west-3.amazonaws.com https://ec2.me-south-1.amazonaws.com https://ec2.sa-east-1.amazonaws.com https://ec2.us-east-1.amazonaws.com https://ec2.us-east-2.amazonaws.com https://ec2.us-west-1.amazonaws.com https://ec2.us-west-2.amazonaws.com https://ec2.af-south-1.amazonaws.com https://iam.amazonaws.com"
        frame_ancestors: "'self'"
        frame_src: "'self' https://gl-staging.freetls.fastly.net https://www.google.com/recaptcha/ https://www.recaptcha.net/ https://content.googleapis.com https://content-cloudresourcemanager.googleapis.com https://content-compute.googleapis.com https://content-cloudbilling.googleapis.com https://*.codesandbox.io https://customers.stg.gitlab.com"
        img_src: "* data: blob:"
        object_src: "'none'"
        report_uri: "https://sentry.gitlab.net/api/15/security/?sentry_key=526a2f38a53d44e3a8e69bfa001d1e8b"
        script_src: "'strict-dynamic' 'self' 'unsafe-inline' 'unsafe-eval' https://gl-staging.freetls.fastly.net https://gl-staging-canary.freetls.fastly.net https://www.google.com/recaptcha/ https://www.gstatic.com/recaptcha/ https://www.recaptcha.net/ https://apis.google.com https://*.pendo.io"
        style_src: "'self' 'unsafe-inline' https://gl-staging.freetls.fastly.net https://gl-staging-canary.freetls.fastly.net"
        worker_src: "https://gl-staging.freetls.fastly.net https://gl-staging-canary.freetls.fastly.net https://staging.gitlab.com blob: data:"
    omniauth:
      providers:
        - secret: gitlab-google-oauth2-v1
        - secret: gitlab-twitter-oauth2-v1
        - secret: gitlab-github-oauth2-v1
        - secret: gitlab-bitbucket-oauth2-v1
        - secret: gitlab-group-saml-oauth2-v1
        - secret: gitlab-salesforce-oauth2-v1
    sidekiq:
      routingRules:
        - ["name=project_import_schedule", null] # we cannot migrate this worker yet, https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/1064
        - ["name=email_receiver,service_desk_email_receiver", null] # we need to change mail_room config to migrate these, https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/1087
        - ["resource_boundary=cpu&urgency=high&tags!=requires_disk_io", null] # urgent-cpu-bound
        - ["resource_boundary=memory", null] # memory-bound
        - ["feature_category=global_search&urgency=throttled", null] # elasticsearch
        - ["resource_boundary!=cpu&urgency=high", null] # urgent-other
        - ["resource_boundary=cpu&urgency=default,low", null] # low-urgency-cpu-bound
        - ["feature_category=database&urgency=throttled", null] # database-throttled
        - ["feature_category=gitaly&urgency=throttled", null] # gitaly-throttled
        - ["name=authorized_project_update:authorized_project_update_user_refresh_from_replica,authorized_project_update:authorized_project_update_user_refresh_with_low_urgency", null] # move this to the sin-bin shard
        - ["*", "default"] # catchall on k8s

  geo:
    enabled: true
    nodeName: {{ .Environment.Values | getOrNil "gitlab_domain" | default "" }}
    role: primary
    registry:
      syncSecret:
        secret: gitlab-registry-notification-v1
    replication:
      enabled: true

  hosts:
    gitlab:
      name: staging.gitlab.com
    kas:
      name: kas.staging.gitlab.com

  psql:
    prepared_statements: false
    load_balancing:
      discover:
        nameserver: <%= File.read('/etc/gitlab/instance_name').strip %>
        record: db-replica.service.consul.
        record_type: SRV
        port: 8600
        use_tcp: true
