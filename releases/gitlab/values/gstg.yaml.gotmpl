---

nginx-ingress:
  controller:
    nodeSelector:
      cloud.google.com/gke-nodepool: default-2

registry:
  database:
    enabled: true
    host: pgbouncer-registry.int.gstg.gitlab.net
    port: 6432
    user: gitlab-registry
    name: gitlabhq_registry
    password:
      secret: registry-postgresql-password-v1
    pool:
      maxopen: 16
      maxidle: 16
      maxlifetime: 5m
  migration:
    enabled: true
    rootdirectory: gitlab
  storage:
    secret: registry-storage-v4
  # When enabled, the upload purger will attempt to make deletes against the
  # common GCS bucket. It is not suitable to be enabled in production until the
  # following issues are resolved:
  # https://gitlab.com/gitlab-org/container-registry/-/issues/216
  # https://gitlab.com/gitlab-org/container-registry/-/issues/217
  maintenance:
    uploadpurging:
      enabled: false
  gc:
    disabled: false
    maxbackoff: 30m
    reviewafter: 30m
  hpa:
    maxReplicas: 20

gitlab:
  mailroom:
    nodeSelector:
      cloud.google.com/gke-nodepool: default-2
  webservice:
    deployments:
      api:
        nodeSelector:
          cloud.google.com/gke-nodepool: api-2
      web:
        extraEnv:
          CUSTOMER_PORTAL_URL: "https://customers.stg.gitlab.com"
          GITLAB_PERFORMANCE_BAR_STATS_URL: "https://nonprod-log.gitlab.net/app/dashboards#/view/8d301370-72b4-11eb-9f15-4952965e0e78?_g=(filters%3A!()%2CrefreshInterval%3A(pause%3A!t%2Cvalue%3A0)%2Ctime%3A(from%3Anow-15m%2Cto%3Anow))"
          GITLAB_TRACING: "opentracing://jaeger?udp_endpoint=localhost%3A6831&sampler=probabilistic&sampler_param=0.01&service_name=web"
          GITLAB_LICENSE_MODE: test # This should be enabled only for staging, see https://gitlab.com/gitlab-com/gl-infra/infrastructure/-/issues/11393#note_631781706
    minReplicas: 2
    maxReplicas: 30
    extraEnv:
      GITLAB_THROTTLE_BYPASS_HEADER: "X-GitLab-RateLimit-Bypass"
      GITLAB_THROTTLE_DRY_RUN: "throttle_unauthenticated_files_api,throttle_authenticated_files_api"
      DISABLE_PUMA_NAKAYOSHI_FORK: "true"
      USE_NEW_LOAD_BALANCER_QUERY: "true"
      GITLAB_PERFORMANCE_BAR_STATS_URL: "https://nonprod-log.gitlab.net/app/dashboards#/view/8d301370-72b4-11eb-9f15-4952965e0e78?_g=(filters%3A!()%2CrefreshInterval%3A(pause%3A!t%2Cvalue%3A0)%2Ctime%3A(from%3Anow-15m%2Cto%3Anow))"
      GITLAB_LOG_DEPRECATIONS: "true"
    workhorse:
      resources:
        limits:
          memory: 2G
        requests:
          cpu: 600m
          memory: 200M
    resources:
      limits:
        memory: 6.0G
      requests:
        cpu: 4
        memory: 5G
  kas:
    image:
      tag: v14.4.0-rc1-race
    nodeSelector:
      cloud.google.com/gke-nodepool: default-2
    workhorse:
      scheme: 'https'
      host: 'int.gstg.gitlab.net'
      port: 11443
    customConfig:
      observability:
        sentry:
          dsn: https://af4940ad841b46d78708595fc654af78@sentry.gitlab.net/124
          environment: {{ .Environment.Values | getOrNil "env_prefix" }}
  sidekiq:
    extraEnv:
      SIDEKIQ_SEMI_RELIABLE_FETCH_TIMEOUT: 5
      USE_GITLAB_LOGGER: 1
      GITLAB_LOG_DEPRECATIONS: "true"
      CUSTOMER_PORTAL_URL: "https://customers.stg.gitlab.com"
    extraInitContainers: |
      - name: write-instance-name
        args:
          - -c
          - echo "$INSTANCE_NAME" > /etc/gitlab/instance_name
        command:
          - sh
        env:
          - name: INSTANCE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
        image: 'busybox:latest'
        volumeMounts:
          - mountPath: /etc/gitlab
            name: sidekiq-secrets
    pods:
      - name: catchall
        common:
          labels:
            shard: catchall
        concurrency: 15
        minReplicas: 1
        maxReplicas: 100
        nodeSelector:
          cloud.google.com/gke-nodepool: catchall-0
        podLabels:
          deployment: sidekiq-catchall
          shard: catchall
        queueSelector: false
        queues: "default,mailers,project_import_schedule"
        resources:
          requests:
            cpu: 800m
            memory: 1G
          limits:
            cpu: 1.5
            memory: 2G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"catchall\"}"
        extraVolumes: |
          # This is needed because of https://gitlab.com/gitlab-org/gitlab/-/issues/330317
          # where temp files are written to `/srv/gitlab/shared`
          - name: sidekiq-shared
            emptyDir:
              sizeLimit: 10G
      - name: imports
        common:
          labels:
            shard: imports
        concurrency: 1
        minReplicas: 2
        maxReplicas: 2
        nodeSelector:
          cloud.google.com/gke-nodepool: catchall-0
        podLabels:
          deployment: sidekiq-imports
          shard: imports
        queueSelector: false
        queues: imports
        resources:
          requests:
            cpu: 800m
            memory: 1G
          limits:
            cpu: 1.5
            memory: 2G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"imports\"}"
      - name: memory-bound
        common:
          labels:
            shard: memory-bound
        concurrency: 1
        minReplicas: 1
        maxReplicas: 16
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-memory-bound-1
        podLabels:
          deployment: sidekiq-memory-bound
          shard: memory-bound
        queueSelector: false
        queues: memory_bound
        resources:
          requests:
            cpu: 500m
            memory: 3G
          limits:
            cpu: 2
            memory: 8G
        extraVolumeMounts: |
          - name: sidekiq-shared
            mountPath: /srv/gitlab/shared
            readOnly: false
        extraVolumes: |
          - name: sidekiq-shared
            emptyDir:
              sizeLimit: 50G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"memory-bound\"}"
      # Run background migrations in their own shard
      - name: database-throttled
        common:
          labels:
            shard: database-throttled
        concurrency: 5 # Discussion on this value in https://gitlab.com/gitlab-com/gl-infra/k8s-workloads/gitlab-com/-/merge_requests/276/diffs#note_367270352
        minReplicas: 1
        maxReplicas: 1
        nodeSelector:
          cloud.google.com/gke-nodepool: default-2
        podLabels:
          deployment: sidekiq-database-throttled
          shard: database-throttled
        queueSelector: false
        queues: database_throttled
        resources:
          requests:
            cpu: 500m
            memory: 3G
          limits:
            cpu: 1.5
            memory: 8G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"database-throttled\"}"
      # Run Gitaly Storage Migrations on their own shards
      # https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/436
      # Allow up to a maximum of 24 concurrent gitaly-throttled jobs
      - name: gitaly-throttled
        common:
          labels:
            shard: gitaly-throttled
        concurrency: 8
        minReplicas: 1
        maxReplicas: 3
        nodeSelector:
          cloud.google.com/gke-nodepool: default-2
        podLabels:
          deployment: sidekiq-gitaly-throttled
          shard: gitaly-throttled
        queueSelector: false
        queues: gitaly_throttled
        resources:
          requests:
            cpu: 500m
            memory: 1300M
          limits:
            cpu: 1.5
            memory: 6G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"gitaly-throttled\"}"
      - name: elasticsearch
        common:
          labels:
            shard: elasticsearch
        concurrency: 2
        minReplicas: 2
        maxReplicas: 2
        nodeSelector:
          cloud.google.com/gke-nodepool: default-2
        podLabels:
          deployment: sidekiq-elasticsearch
          shard: elasticsearch
        queueSelector: false
        queues: elasticsearch
        resources:
          requests:
            cpu: 800m
            memory: 2G
          limits:
            cpu: 2
            memory: 8G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"elasticsearch\"}"
      - name: low-urgency-cpu-bound
        common:
          labels:
            shard: low-urgency-cpu-bound
        concurrency: 5
        minReplicas: 2
        maxReplicas: 10
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-low-urgency-cpu-bound-1
        podLabels:
          deployment: sidekiq-low-urgency-cpu-bound
          shard: low-urgency-cpu-bound
        queueSelector: false
        queues: low_urgency_cpu_bound
        resources:
          requests:
            cpu: 1
            memory: 5G
          limits:
            cpu: 1.5
            memory: 6G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"low-urgency-cpu-bound\"}"
      # This is a shard that segregates seemingly problematic queues from the rest of our infrastructure
      - name: quarantine
        common:
          labels:
            shard: quarantine
        concurrency: 15
        minReplicas: 1
        maxReplicas: 50
        nodeSelector:
          cloud.google.com/gke-nodepool: catchall-0
        podLabels:
          deployment: sidekiq-quarantine
          shard: quarantine
        queueSelector: false
        queues: quarantine
        resources:
          requests:
            cpu: 800m
            memory: 1G
          limits:
            cpu: 1.5
            memory: 2G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"quarantine\"}"
      - name: urgent-cpu-bound
        common:
          labels:
            shard: urgent-cpu-bound
        concurrency: 5
        minReplicas: 1
        maxReplicas: 10
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-urgent-cpu-bound-1
        podLabels:
          deployment: sidekiq-urgent-cpu-bound
          shard: urgent-cpu-bound
        queueSelector: false
        queues: urgent_cpu_bound
        resources:
          requests:
            cpu: 650m
            memory: 2.5G
          limits:
            cpu: 1
            memory: 3.1G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"urgent-cpu-bound\"}"
      - name: urgent-other
        common:
          labels:
            shard: urgent-other
        concurrency: 5
        minReplicas: 5
        maxReplicas: 10
        nodeSelector:
          cloud.google.com/gke-nodepool: sidekiq-urgent-other-1
        podLabels:
          deployment: sidekiq-urgent-other
          shard: urgent-other
        queueSelector: false
        queues: urgent_other,email_receiver,service_desk_email_receiver
        resources:
          requests:
            cpu: 500m
            memory: 2G
          limits:
            cpu: 1.5
            memory: 3G
        extraEnv:
          GITLAB_SENTRY_EXTRA_TAGS: "{\"type\": \"sidekiq\", \"stage\": \"main\", \"shard\": \"urgent-other\"}"
    psql:
      host: pgbouncer-sidekiq.int.gstg.gitlab.net
      load_balancing:
        discover:
          nameserver: <%= File.read('/etc/gitlab/instance_name').strip %>
          record: db-replica.service.consul.
          record_type: SRV
          port: 8600
          use_tcp: true

global:
  appConfig:
    contentSecurityPolicy:
      enabled: true
      report_only: false
      directives:
        connect_src: "'self' https://staging.gitlab.com https://gl-staging.freetls.fastly.net https://gl-staging-canary.freetls.fastly.net wss://gitlab.com wss://staging.gitlab.com wss://gstg.gitlab.com https://sentry.gitlab.net https://customers.stg.gitlab.com https://snowplow.trx.gitlab.net https://ec2.ap-east-1.amazonaws.com https://ec2.ap-northeast-1.amazonaws.com https://ec2.ap-northeast-2.amazonaws.com https://ec2.ap-northeast-3.amazonaws.com https://ec2.ap-south-1.amazonaws.com https://ec2.ap-southeast-1.amazonaws.com https://ec2.ap-southeast-2.amazonaws.com https://ec2.ca-central-1.amazonaws.com https://ec2.eu-central-1.amazonaws.com https://ec2.eu-north-1.amazonaws.com https://ec2.eu-west-1.amazonaws.com https://ec2.eu-west-2.amazonaws.com https://ec2.eu-west-3.amazonaws.com https://ec2.me-south-1.amazonaws.com https://ec2.sa-east-1.amazonaws.com https://ec2.us-east-1.amazonaws.com https://ec2.us-east-2.amazonaws.com https://ec2.us-west-1.amazonaws.com https://ec2.us-west-2.amazonaws.com https://ec2.af-south-1.amazonaws.com https://iam.amazonaws.com"
        frame_ancestors: "'self'"
        frame_src: "'self' https://gl-staging.freetls.fastly.net https://www.google.com/recaptcha/ https://www.recaptcha.net/ https://content.googleapis.com https://content-cloudresourcemanager.googleapis.com https://content-compute.googleapis.com https://content-cloudbilling.googleapis.com https://*.codesandbox.io https://customers.stg.gitlab.com"
        img_src: "* data: blob:"
        object_src: "'none'"
        report_uri: "https://sentry.gitlab.net/api/15/security/?sentry_key=526a2f38a53d44e3a8e69bfa001d1e8b"
        script_src: "'strict-dynamic' 'self' 'unsafe-inline' 'unsafe-eval' https://gl-staging.freetls.fastly.net https://gl-staging-canary.freetls.fastly.net https://www.google.com/recaptcha/ https://www.gstatic.com/recaptcha/ https://www.recaptcha.net/ https://apis.google.com https://*.pendo.io"
        style_src: "'self' 'unsafe-inline' https://gl-staging.freetls.fastly.net https://gl-staging-canary.freetls.fastly.net"
        worker_src: "https://gl-staging.freetls.fastly.net https://gl-staging-canary.freetls.fastly.net https://staging.gitlab.com blob: data:"
    omniauth:
      providers:
        - secret: gitlab-google-oauth2-v1
        - secret: gitlab-twitter-oauth2-v1
        - secret: gitlab-github-oauth2-v1
        - secret: gitlab-bitbucket-oauth2-v1
        - secret: gitlab-group-saml-oauth2-v1
        - secret: gitlab-salesforce-oauth2-v1
    sidekiq:
      routingRules:
        # We cannot migrate these workers yet: https://gitlab.com/gitlab-org/gitlab/-/issues/340630 / https://gitlab.com/gitlab-com/gl-infra/scalability/-/issues/1263 / https://gitlab.com/gitlab-org/gitlab/-/issues/340629
        - ["tags=needs_own_queue", null]
        - ["worker_name=AuthorizedProjectUpdate::UserRefreshFromReplicaWorker,AuthorizedProjectUpdate::UserRefreshWithLowUrgencyWorker", "quarantine"] # move this to the quarantine shard
        - ["worker_name=RepositoryImportWorker", "imports"] # imports
        - ["resource_boundary=cpu&urgency=high", "urgent_cpu_bound"] # urgent-cpu-bound
        - ["resource_boundary=memory", "memory_bound"] # memory-bound
        - ["feature_category=global_search&urgency=throttled", "elasticsearch"] # elasticsearch
        - ["resource_boundary!=cpu&urgency=high", "urgent_other"] # urgent-other
        - ["resource_boundary=cpu&urgency=default,low", "low_urgency_cpu_bound"] # low-urgency-cpu-bound
        - ["feature_category=database&urgency=throttled", "database_throttled"] # database-throttled
        - ["feature_category=gitaly&urgency=throttled", "gitaly_throttled"] # gitaly-throttled
        - ["*", "default"] # catchall on k8s

  email:
    reply_to: noreply@staging.gitlab.com

  geo:
    enabled: true
    nodeName: {{ .Environment.Values | getOrNil "gitlab_domain" | default "" }}
    role: primary
    registry:
      syncSecret:
        secret: gitlab-registry-notification-v1
    replication:
      enabled: true

  hosts:
    gitlab:
      name: staging.gitlab.com
    kas:
      name: kas.staging.gitlab.com

  psql:
    prepared_statements: false
    load_balancing:
      discover:
        nameserver: <%= File.read('/etc/gitlab/instance_name').strip %>
        record: db-replica.service.consul.
        record_type: SRV
        port: 8600
        use_tcp: true
  tracing:
    urlTemplate: 'https://jaeger.gstg.gitlab.net/search?service={{ "{{" }} service {{ "}}" }}&tags=%7B{{ "\"" }}correlation_id{{ "\"" }}%3A{{ "\""}}{{ "{{" }} correlation_id {{ "}}" }}{{ "\"" }}%7D'
